{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFB7KkiWC3GDpu1ULam39A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worldofaryavart/colab_notebooks/blob/colabnotebook/query_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers spacy wordnet nltk"
      ],
      "metadata": {
        "id": "PSlH6zaYN7GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "VKvBzwAUOGQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgf95SwXNzN9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "7FmNFVuLOA6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.texts[idx])\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_length,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'labels': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "7ClslmmEOJg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntentClassifier(nn.Module):\n",
        "  def __init__(self, n_classes, pretrained_model=\"bert-base-uncased\"):\n",
        "    super().__init__()\n",
        "    self.bert = AutoModel.from_pretrained(pretrained_model)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(outputs[1])\n",
        "    return self.fc(output)"
      ],
      "metadata": {
        "id": "ovSA5YevOLkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryExpander:\n",
        "  def __init__(self):\n",
        "    self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "  def get_synonyms(self, word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "      for lemma in syn.lemmas():\n",
        "        synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "  def expand_query(self, query):\n",
        "    doc = self.nlp(query)\n",
        "    expanded_terms = []\n",
        "\n",
        "    for token in doc:\n",
        "      if token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
        "        synonyms = self.get_synonyms(token.text)\n",
        "        expanded_terms.extend(synonyms[:2])\n",
        "\n",
        "    return list(set([term.lower() for term in expanded_terms]))\n"
      ],
      "metadata": {
        "id": "wFiIe6XfOONw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_dataset():\n",
        "  \"\"\"Create a sample dataset for intent classification\"\"\"\n",
        "  queries = [\n",
        "      \"Find reasearch papers about quantum computing\",\n",
        "      \"Download PDF papers on machine learning\",\n",
        "      \"Summarize recent articles about AI\",\n",
        "      \"Show me videos explaining neural networks\",\n",
        "      \"Get images of black holes\",\n",
        "  ]\n",
        "\n",
        "  intents = [\n",
        "        \"research_retrieval\",\n",
        "        \"pdf_download\",\n",
        "        \"summarization\",\n",
        "        \"video_search\",\n",
        "        \"image_search\",\n",
        "        # Add corresponding intents...\n",
        "  ]\n",
        "\n",
        "  return pd.DataFrame({'query': queries, 'intent': intents})\n"
      ],
      "metadata": {
        "id": "Pa_IuZ43OQMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_intent_classifier(model, train_loader, device, epochs=3):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "kggUmtBWOSWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  df = create_sample_dataset()\n",
        "  label_encoder = LabelEncoder()\n",
        "  df['encoded_intent'] = label_encoder.fit_transform(df['intent'])\n",
        "\n",
        "  train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "  n_classes = len(label_encoder.classes_)\n",
        "  model = IntentClassifier(n_classes)\n",
        "\n",
        "  train_dataset = QueryDataset(\n",
        "        texts=train_df['query'].values,\n",
        "        labels=train_df['encoded_intent'].values,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=8,\n",
        "      shuffle=True\n",
        "  )\n",
        "\n",
        "  query_expander = QueryExpander()\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "\n",
        "  train_intent_classifier(model, train_loader, device)\n",
        "\n",
        "  test_query = \"Find recent papers about deep learning\"\n",
        "\n",
        "  expanded_terms = query_expander.expand_query(test_query)\n",
        "  print(f\"Expanded terms: {expanded_terms}\")\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(\n",
        "        test_query,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids=encoding['input_ids'].to(device),\n",
        "        attention_mask=encoding['attention_mask'].to(device)\n",
        "    )\n",
        "    predicted_intent = label_encoder.inverse_transform([outputs.argmax().item()])[0]\n",
        "    print(f\"Predicted intent: {predicted_intent}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "_YmaKxTYOVUt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}