{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worldofaryavart/colab_notebooks/blob/colabnotebook/making_scraperModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers spacy wordnet nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O15ybFBEF0at",
        "outputId": "5dcab2aa-5220-4200-abc5-1ad98e87ea80"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting wordnet\n",
            "  Downloading wordnet-0.0.1b2.tar.gz (8.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Collecting colorama==0.3.9 (from wordnet)\n",
            "  Downloading colorama-0.3.9-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading colorama-0.3.9-py2.py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: wordnet\n",
            "  Building wheel for wordnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordnet: filename=wordnet-0.0.1b2-py3-none-any.whl size=10498 sha256=6133a2650f933e092ebb79d93a043d002c3f2a292926d7224f6f0d5e845c5719\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/a1/e8/4649c8712033dcdbd1e64a0fc75216a5d1769665852c36b4f9\n",
            "Successfully built wordnet\n",
            "Installing collected packages: colorama, wordnet\n",
            "Successfully installed colorama-0.3.9 wordnet-0.0.1b2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "UYsdBOEtF78Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGNig5jGGZZF",
        "outputId": "beed6d7c-4e01-4535-fa35-80bfc6d8708b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqGU7y54G9Bj",
        "outputId": "19c879a9-4e3a-43a6-b70b-2e45855d7632"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.texts[idx])\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_length,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'labels': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "Ji6nyLteHFOb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntentClassifier(nn.Module):\n",
        "  def __init__(self, n_classes, pretrained_model=\"bert-base-uncased\"):\n",
        "    super().__init__()\n",
        "    self.bert = AutoModel.from_pretrained(pretrained_model)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(outputs[1])\n",
        "    return self.fc(output)"
      ],
      "metadata": {
        "id": "VXgwyfu_PJI-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryExpander:\n",
        "  def __init__(self):\n",
        "    self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "  def get_synonyms(self, word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "      for lemma in syn.lemmas():\n",
        "        synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "  def expand_query(self, query):\n",
        "    doc = self.nlp(query)\n",
        "    expanded_terms = []\n",
        "\n",
        "    for token in doc:\n",
        "      if token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
        "        synonyms = self.get_synonyms(token.text)\n",
        "        expanded_terms.extend(synonyms[:2])\n",
        "\n",
        "    return list(set([term.lower() for term in expanded_terms]))\n"
      ],
      "metadata": {
        "id": "f5pYszga8iiv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_dataset():\n",
        "  \"\"\"Create a sample dataset for intent classification\"\"\"\n",
        "  queries = [\n",
        "      \"Find reasearch papers about quantum computing\",\n",
        "      \"Download PDF papers on machine learning\",\n",
        "      \"Summarize recent articles about AI\",\n",
        "      \"Show me videos explaining neural networks\",\n",
        "      \"Get images of black holes\",\n",
        "  ]\n",
        "\n",
        "  intents = [\n",
        "        \"research_retrieval\",\n",
        "        \"pdf_download\",\n",
        "        \"summarization\",\n",
        "        \"video_search\",\n",
        "        \"image_search\",\n",
        "        # Add corresponding intents...\n",
        "  ]\n",
        "\n",
        "  return pd.DataFrame({'query': queries, 'intent': intents})\n"
      ],
      "metadata": {
        "id": "8mnmfPfk-iei"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_intent_classifier(model, train_loader, device, epochs=3):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "oG_t4J1g_i4X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  df = create_sample_dataset()\n",
        "  label_encoder = LabelEncoder()\n",
        "  df['encoded_intent'] = label_encoder.fit_transform(df['intent'])\n",
        "\n",
        "  train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "  n_classes = len(label_encoder.classes_)\n",
        "  model = IntentClassifier(n_classes)\n",
        "\n",
        "  train_dataset = QueryDataset(\n",
        "        texts=train_df['query'].values,\n",
        "        labels=train_df['encoded_intent'].values,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=8,\n",
        "      shuffle=True\n",
        "  )\n",
        "\n",
        "  query_expander = QueryExpander()\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "\n",
        "  train_intent_classifier(model, train_loader, device)\n",
        "\n",
        "  test_query = \"Find recent papers about deep learning\"\n",
        "\n",
        "  expanded_terms = query_expander.expand_query(test_query)\n",
        "  print(f\"Expanded terms: {expanded_terms}\")\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(\n",
        "        test_query,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids=encoding['input_ids'].to(device),\n",
        "        attention_mask=encoding['attention_mask'].to(device)\n",
        "    )\n",
        "    predicted_intent = label_encoder.inverse_transform([outputs.argmax().item()])[0]\n",
        "    print(f\"Predicted intent: {predicted_intent}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfEyPCuwBn7a",
        "outputId": "851b15af-3b3e-4bc2-885b-655044606055"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 1.6312\n",
            "Epoch 2, Average Loss: 1.4880\n",
            "Epoch 3, Average Loss: 1.5727\n",
            "Expanded terms: ['discovery', 'document', 'oceanic_abyss', 'report', 'abstruse', 'scholarship', 'breakthrough', 'larn', 'holocene', 'holocene_epoch']\n",
            "Predicted intent: image_search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright beautifulsoup4 pytesseract pillow PyMuPDF youtube_dl transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y98lugkCaKkx",
        "outputId": "01a9e049-0d7f-42e2-f7c7-8ef39e196cdc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.48.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting youtube_dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.10/dist-packages (from playwright) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading playwright-1.48.0-py3-none-manylinux1_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_dl, pytesseract, PyMuPDF, pyee, playwright\n",
            "Successfully installed PyMuPDF-1.24.13 playwright-1.48.0 pyee-12.0.0 pytesseract-0.3.13 youtube_dl-2021.12.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import aiohttp\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import fitz\n",
        "import youtube_dl\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "import threading\n",
        "from queue import Queue\n",
        "import hashlib\n",
        "import time\n"
      ],
      "metadata": {
        "id": "ybCQuWaAJthn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SuperPoweredCrawler:\n",
        "  def __init__(self):\n",
        "    self.visited_urls = set()\n",
        "    self.url_queue = Queue()\n",
        "    self.results = []\n",
        "    self.content_lock = threading.Lock()\n",
        "\n",
        "    self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    self.max_depth = 3\n",
        "    self.max_pages_per_domain = 1000\n",
        "    self.respect_robots = False\n",
        "    self.concurrent_requests = 50\n",
        "\n",
        "    self.content_handlers = {\n",
        "        'text/plain': self.process_html,\n",
        "        'application/pdf': self.process_pdf,\n",
        "        'image': self.process_image,\n",
        "        'video': self.process_video\n",
        "    }\n",
        "\n",
        "  async def start_crawl(self, seed_urls, search_query):\n",
        "    \"\"\"Start the crawling process with mutliple seed URLs\"\"\"\n",
        "\n",
        "    print(f\"Starting crawl for query: {search_query}\")\n",
        "\n",
        "    for url in seed_urls:\n",
        "      self.url_queue.put((url, 0))\n",
        "\n",
        "    async with async_playwright() as playwright:\n",
        "      browser = await playwright.chromium.launch(headless=True)\n",
        "\n",
        "      contexts = [await browser.new_context() for _ in range(self.concurrent_requests)]\n",
        "\n",
        "      tasks = []\n",
        "      for context in contexts:\n",
        "        task = asyncio.create_task(self.crawler_worker(context, search_query))\n",
        "        tasks.append(task)\n",
        "\n",
        "      await asyncio.gather(*tasks)\n",
        "      await browser.close()\n",
        "\n",
        "    return self.results\n",
        "\n",
        "  async def crawler_worker(self, context, search_query):\n",
        "    \"\"\"Worker process for crawling pages\"\"\"\n",
        "    while not self.url_queue.empty():\n",
        "      try:\n",
        "        url, depth = self.url_queue.get_nowait()\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      if depth > self.max_depth or url in self.visited_urls:\n",
        "        continue\n",
        "\n",
        "      try:\n",
        "        page = await context.new_page()\n",
        "        await page.goto(url, wait_until='networkidle', timeout=40000)\n",
        "\n",
        "        # Get page content and metadata\n",
        "        content = await page.content()\n",
        "        title = await page.title()\n",
        "\n",
        "        #Determine content type\n",
        "        content_type = await self.detect_content_type(page)\n",
        "\n",
        "        #Process content based ont type\n",
        "        if content_type in self.content_handlers:\n",
        "          result = await self.content_handlers[content_type](page, content, url)\n",
        "          if results:\n",
        "            self.add_result(result, search_query)\n",
        "\n",
        "        new_urls = await self.extract_urls(page)\n",
        "        for new_url in new_urls:\n",
        "          if self.should_crawl(new_url):\n",
        "            self.url_queue.put((new_url, depth + 1))\n",
        "\n",
        "        await page.close()\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing URL {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "      self.visited_urls.add(url)\n",
        "\n",
        "  async def detect_content_type(self, page):\n",
        "    \"\"\"Detect the type of content on the page\"\"\"\n",
        "\n",
        "    #Check response headers\n",
        "    response = await page.main_frame.request.response()\n",
        "    if response:\n",
        "      content_type = response.headers.get('content-type', '')\n",
        "\n",
        "      if 'pdf' in content_type:\n",
        "        return 'application/pdf'\n",
        "      elif any(img_type in content_type for image_type in ['image/jpeg', 'image/png', 'image/gif']):\n",
        "        return 'image'\n",
        "      elif 'video' in content_type:\n",
        "        return 'video'\n",
        "\n",
        "    return 'text/html'\n",
        "\n",
        "  async def process_html(self, page, content, url):\n",
        "    \"\"\"Process HTML content and extract relevant information\"\"\"\n",
        "\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    text_content = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "\n",
        "    meta_tags = soup.find_all('meta')\n",
        "    metadata = {\n",
        "        tag.get('name', tag.get('property', '')): tag.get('content', '')\n",
        "        for tag in meta_tags\n",
        "    }\n",
        "\n",
        "    main_content = await self.extract_main_content(page)\n",
        "\n",
        "    summary = None\n",
        "    if len(main_content) > 500:\n",
        "      summary = self.summarization(main_content, max_length = 150, min_length=50)[0]['summary_text']\n",
        "\n",
        "    return {\n",
        "        'url': url,\n",
        "        'type': 'html',\n",
        "        'title': await page.title(),\n",
        "        'content': main_content,\n",
        "        'summary': summary,\n",
        "        'metadata': metadata,\n",
        "        'timestamp': time.time()\n",
        "    }\n",
        "\n",
        "  async def process_pdf(self, page, content, url):\n",
        "    \"\"\"Extract and process PDF content\"\"\"\n",
        "\n",
        "    try:\n",
        "      response = await page.main_frame.request.response()\n",
        "      pdf_content = await response.body()\n",
        "\n",
        "      #Process PDF using PyMuPDF\n",
        "      pdf_document = fitz.open(stream=pdf_content, filetype='pdf')\n",
        "      text_content = \"\"\n",
        "      images = []\n",
        "\n",
        "      for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document[page_num]\n",
        "        text_content += page.get_text()\n",
        "\n",
        "        image_list = page.get_images(full=True)\n",
        "        for img_index, img in enumerate(image_list):\n",
        "          xref = img[0]\n",
        "          base_image = pdf_document.extract_image(xref)\n",
        "          image_bytes = base_image[\"image\"]\n",
        "          images.append(image_bytes)\n",
        "\n",
        "      return {\n",
        "          'url': url,\n",
        "          'type': 'html',\n",
        "          'title': await page.title(),\n",
        "          'content': text_content,\n",
        "          'images': images,\n",
        "          'timestamp': time.time()\n",
        "      }\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing PDF {url}: {e}\")\n",
        "      return None\n",
        "\n",
        "  async def process_image(self, page, content, url):\n",
        "    \"\"\"Process and analyze image content\"\"\"\n",
        "\n",
        "    try:\n",
        "      #Get image data\n",
        "      response = await page.main_frame.request.response()\n",
        "      image_data = await response.body()\n",
        "\n",
        "      #Convert to PIL Image\n",
        "      image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "      ocr_text = pytesseract.image_to_string(image)\n",
        "\n",
        "      #Basic image analysis\n",
        "      width, height = image.size\n",
        "      format = image.format\n",
        "      mode = image.mode\n",
        "\n",
        "      return {\n",
        "          'url': url,\n",
        "          'type': 'image',\n",
        "          'ocr_text': ocr_text,\n",
        "          'metadata': {\n",
        "              'width': width,\n",
        "              'height': height,\n",
        "              'format': format,\n",
        "              'mode': mode\n",
        "          },\n",
        "          'timestamp': time.time()\n",
        "      }\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing image {url}: {str(e)}\")\n",
        "      return None\n",
        "\n",
        "  async def process_video(self, page, content, url):\n",
        "    \"\"\"Extract information from video content\"\"\"\n",
        "\n",
        "    try:\n",
        "      ydl_opts = {\n",
        "          'format': 'best',\n",
        "          'extract_flat': True,\n",
        "          'force_generic_extractor': True\n",
        "      }\n",
        "\n",
        "      with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "        video_info = ydl.extract_info(url, download=False)\n",
        "\n",
        "      return {\n",
        "          'url': url,\n",
        "          'type': 'video',\n",
        "          'title': video_info.get('title'),\n",
        "          'description': video_info.get('description'),\n",
        "          'duration': video_info.get('duration'),\n",
        "          'timestamp': time.time()\n",
        "      }\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing video {url}: {str(e)}\")\n",
        "      return None\n",
        "\n",
        "  def add_result(self, result, search_query):\n",
        "    \"\"\"Add processed result to the results list with relevance scoring\"\"\"\n",
        ""
      ],
      "metadata": {
        "id": "GZlqJEZfaFMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}