{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worldofaryavart/colab_notebooks/blob/colabnotebook/making_scraperModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright beautifulsoup4 pytesseract pillow PyMuPDF youtube_dl transformers\n",
        "!playwright install chromium\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install nest_asyncio\n",
        "!pip install duckduckgo_search\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y98lugkCaKkx",
        "outputId": "aa54dfc4-3bfd-485b-cbed-dfd8e566efa4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.48.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting youtube_dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.10/dist-packages (from playwright) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading playwright-1.48.0-py3-none-manylinux1_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_dl, pytesseract, PyMuPDF, pyee, playwright\n",
            "Successfully installed PyMuPDF-1.24.13 playwright-1.48.0 pyee-12.0.0 pytesseract-0.3.13 youtube_dl-2021.12.17\n",
            "Downloading Chromium 130.0.6723.31 (playwright build v1140)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1140/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 0% 21.1s\u001b[0K\u001b[1G164.5 MiB [] 0% 16.0s\u001b[0K\u001b[1G164.5 MiB [] 0% 10.5s\u001b[0K\u001b[1G164.5 MiB [] 0% 8.5s\u001b[0K\u001b[1G164.5 MiB [] 0% 7.9s\u001b[0K\u001b[1G164.5 MiB [] 1% 7.0s\u001b[0K\u001b[1G164.5 MiB [] 1% 6.4s\u001b[0K\u001b[1G164.5 MiB [] 1% 6.5s\u001b[0K\u001b[1G164.5 MiB [] 2% 6.1s\u001b[0K\u001b[1G164.5 MiB [] 2% 6.3s\u001b[0K\u001b[1G164.5 MiB [] 2% 6.2s\u001b[0K\u001b[1G164.5 MiB [] 3% 6.4s\u001b[0K\u001b[1G164.5 MiB [] 3% 6.0s\u001b[0K\u001b[1G164.5 MiB [] 3% 5.7s\u001b[0K\u001b[1G164.5 MiB [] 3% 6.0s\u001b[0K\u001b[1G164.5 MiB [] 4% 6.1s\u001b[0K\u001b[1G164.5 MiB [] 4% 6.2s\u001b[0K\u001b[1G164.5 MiB [] 4% 6.8s\u001b[0K\u001b[1G164.5 MiB [] 4% 6.4s\u001b[0K\u001b[1G164.5 MiB [] 5% 6.2s\u001b[0K\u001b[1G164.5 MiB [] 5% 5.9s\u001b[0K\u001b[1G164.5 MiB [] 6% 5.8s\u001b[0K\u001b[1G164.5 MiB [] 6% 5.6s\u001b[0K\u001b[1G164.5 MiB [] 6% 5.5s\u001b[0K\u001b[1G164.5 MiB [] 7% 5.4s\u001b[0K\u001b[1G164.5 MiB [] 7% 5.2s\u001b[0K\u001b[1G164.5 MiB [] 8% 5.0s\u001b[0K\u001b[1G164.5 MiB [] 9% 5.0s\u001b[0K\u001b[1G164.5 MiB [] 9% 4.8s\u001b[0K\u001b[1G164.5 MiB [] 10% 4.8s\u001b[0K\u001b[1G164.5 MiB [] 10% 4.7s\u001b[0K\u001b[1G164.5 MiB [] 11% 4.7s\u001b[0K\u001b[1G164.5 MiB [] 11% 4.6s\u001b[0K\u001b[1G164.5 MiB [] 12% 4.5s\u001b[0K\u001b[1G164.5 MiB [] 12% 4.6s\u001b[0K\u001b[1G164.5 MiB [] 13% 4.6s\u001b[0K\u001b[1G164.5 MiB [] 13% 4.7s\u001b[0K\u001b[1G164.5 MiB [] 14% 4.6s\u001b[0K\u001b[1G164.5 MiB [] 14% 4.5s\u001b[0K\u001b[1G164.5 MiB [] 15% 4.4s\u001b[0K\u001b[1G164.5 MiB [] 16% 4.3s\u001b[0K\u001b[1G164.5 MiB [] 17% 4.1s\u001b[0K\u001b[1G164.5 MiB [] 17% 4.0s\u001b[0K\u001b[1G164.5 MiB [] 18% 4.0s\u001b[0K\u001b[1G164.5 MiB [] 18% 3.8s\u001b[0K\u001b[1G164.5 MiB [] 19% 3.8s\u001b[0K\u001b[1G164.5 MiB [] 20% 3.7s\u001b[0K\u001b[1G164.5 MiB [] 21% 3.6s\u001b[0K\u001b[1G164.5 MiB [] 21% 3.5s\u001b[0K\u001b[1G164.5 MiB [] 22% 3.4s\u001b[0K\u001b[1G164.5 MiB [] 23% 3.3s\u001b[0K\u001b[1G164.5 MiB [] 24% 3.2s\u001b[0K\u001b[1G164.5 MiB [] 25% 3.1s\u001b[0K\u001b[1G164.5 MiB [] 26% 3.1s\u001b[0K\u001b[1G164.5 MiB [] 26% 3.0s\u001b[0K\u001b[1G164.5 MiB [] 27% 2.9s\u001b[0K\u001b[1G164.5 MiB [] 28% 2.9s\u001b[0K\u001b[1G164.5 MiB [] 29% 2.8s\u001b[0K\u001b[1G164.5 MiB [] 30% 2.7s\u001b[0K\u001b[1G164.5 MiB [] 31% 2.6s\u001b[0K\u001b[1G164.5 MiB [] 32% 2.6s\u001b[0K\u001b[1G164.5 MiB [] 33% 2.5s\u001b[0K\u001b[1G164.5 MiB [] 34% 2.5s\u001b[0K\u001b[1G164.5 MiB [] 34% 2.4s\u001b[0K\u001b[1G164.5 MiB [] 35% 2.4s\u001b[0K\u001b[1G164.5 MiB [] 36% 2.3s\u001b[0K\u001b[1G164.5 MiB [] 37% 2.2s\u001b[0K\u001b[1G164.5 MiB [] 38% 2.2s\u001b[0K\u001b[1G164.5 MiB [] 39% 2.2s\u001b[0K\u001b[1G164.5 MiB [] 39% 2.1s\u001b[0K\u001b[1G164.5 MiB [] 40% 2.1s\u001b[0K\u001b[1G164.5 MiB [] 41% 2.0s\u001b[0K\u001b[1G164.5 MiB [] 42% 2.0s\u001b[0K\u001b[1G164.5 MiB [] 43% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 44% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 45% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 45% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 46% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 47% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 48% 1.7s\u001b[0K\u001b[1G164.5 MiB [] 49% 1.7s\u001b[0K\u001b[1G164.5 MiB [] 50% 1.6s\u001b[0K\u001b[1G164.5 MiB [] 51% 1.6s\u001b[0K\u001b[1G164.5 MiB [] 51% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 52% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 53% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 54% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 55% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 56% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 56% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 57% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 58% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 59% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 59% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 60% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 61% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 62% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 63% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 64% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 65% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 66% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 67% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 68% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 69% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 70% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 71% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 72% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 73% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 73% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 74% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 75% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 76% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 77% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 78% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 79% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 80% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 81% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 82% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 83% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 84% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 85% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 86% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 87% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 88% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 89% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 90% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 91% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 92% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 93% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 94% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 97% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 130.0.6723.31 (playwright build v1140) downloaded to /root/.cache/ms-playwright/chromium-1140\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 5% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 15% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 22% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 28% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 35% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 56% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 87% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 1s (183 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (2,711 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123653 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting duckduckgo_search\n",
            "  Downloading duckduckgo_search-6.3.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo_search) (8.1.7)\n",
            "Collecting primp>=0.6.5 (from duckduckgo_search)\n",
            "  Downloading primp-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Downloading duckduckgo_search-6.3.3-py3-none-any.whl (27 kB)\n",
            "Downloading primp-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo_search\n",
            "Successfully installed duckduckgo_search-6.3.3 primp-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from asyncio import Queue\n",
        "from bs4 import BeautifulSoup\n",
        "from playwright.async_api import async_playwright\n",
        "from transformers import pipeline\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "import io\n",
        "import aiohttp\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import warnings\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "ybCQuWaAJthn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "import aiohttp\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from urllib.robotparser import RobotFileParser\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "L-ZgI6xp4tSB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "y6SJKrkkkpx9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SuperPoweredCrawler:\n",
        "    def __init__(self):\n",
        "        self.visited_urls = set()\n",
        "        self.url_queue = asyncio.Queue()  # Changed to asyncio.Queue\n",
        "        self.results = []\n",
        "        self.content_lock = threading.Lock()\n",
        "        self.browser = None\n",
        "        self.context = None  # Added context initialization\n",
        "\n",
        "        # Initialize content extractors\n",
        "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=\"cpu\")\n",
        "        self.min_text_for_summary = 200\n",
        "\n",
        "        # Configurable crawler settings\n",
        "        self.max_depth = 3\n",
        "        self.max_pages = 50\n",
        "        self.max_pages_per_domain = 10\n",
        "        self.concurrent_requests = 3\n",
        "\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "        }\n",
        "\n",
        "    async def initialize_browser(self):\n",
        "        \"\"\"Initialize browser and context\"\"\"\n",
        "        playwright = await async_playwright().start()\n",
        "        self.browser = await playwright.chromium.launch(headless=True)  # Fixed typo in chromium\n",
        "        self.context = await self.browser.new_context(\n",
        "            viewport={'width': 1200, 'height': 800},\n",
        "            user_agent=self.headers['User-Agent'],\n",
        "            extra_http_headers=self.headers  # Fixed parameter name\n",
        "        )\n",
        "        return playwright\n",
        "\n",
        "    async def cleanup(self, playwright):\n",
        "        \"\"\"Cleanup browser resources\"\"\"\n",
        "        if self.context:\n",
        "            await self.context.close()\n",
        "        if self.browser:\n",
        "            await self.browser.close()\n",
        "        await playwright.stop()\n",
        "\n",
        "    async def start_crawl(self, seed_urls, search_query):\n",
        "        \"\"\"Start the crawling process with multiple seed URLs\"\"\"\n",
        "        print(f\"Starting crawl for query: {search_query}\")\n",
        "\n",
        "        # Initialize the queue with seed URLs\n",
        "        for url in seed_urls:\n",
        "            await self.url_queue.put((url, 0))\n",
        "\n",
        "        try:\n",
        "            playwright = await self.initialize_browser()\n",
        "            workers = [self.crawler_worker(search_query) for _ in range(self.concurrent_requests)]\n",
        "            await asyncio.gather(*workers)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during crawling: {str(e)}\")\n",
        "\n",
        "        finally:\n",
        "            await self.cleanup(playwright)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    async def crawler_worker(self, search_query):\n",
        "        \"\"\"Worker process for crawling pages\"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                if len(self.visited_urls) >= self.max_pages:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    url, depth = await asyncio.wait_for(self.url_queue.get(), timeout=10)\n",
        "                except asyncio.TimeoutError:\n",
        "                    break\n",
        "\n",
        "                if depth > self.max_depth or url in self.visited_urls:\n",
        "                    self.url_queue.task_done()\n",
        "                    continue\n",
        "\n",
        "                print(f\"Crawling: {url}\")\n",
        "\n",
        "                try:\n",
        "                    page = await self.context.new_page()\n",
        "                    response = await page.goto(\n",
        "                        url,\n",
        "                        wait_until='domcontentloaded',  # Fixed typo\n",
        "                        timeout=30000\n",
        "                    )\n",
        "\n",
        "                    if not response:\n",
        "                        print(f\"No response from {url}\")\n",
        "                        await page.close()\n",
        "                        self.url_queue.task_done()\n",
        "                        continue\n",
        "\n",
        "                    if response.status >= 400:\n",
        "                        print(f\"Error response from {url}: {response.status}\")\n",
        "                        await page.close()\n",
        "                        self.url_queue.task_done()\n",
        "                        continue\n",
        "\n",
        "                    content = await page.content()\n",
        "                    result = await self.process_page(page, response, content, url)  # Fixed parameter order\n",
        "                    if result:\n",
        "                        self.add_result(result, search_query)\n",
        "\n",
        "                    if len(self.visited_urls) < self.max_pages:\n",
        "                        new_urls = await self.extract_urls(page)\n",
        "                        for new_url in new_urls:\n",
        "                            if self.should_crawl(new_url):\n",
        "                                await self.url_queue.put((new_url, depth + 1))\n",
        "\n",
        "                    self.visited_urls.add(url)\n",
        "                    await page.close()\n",
        "                    await asyncio.sleep(2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {url}: {str(e)}\")\n",
        "                    if 'page' in locals():\n",
        "                        await page.close()\n",
        "\n",
        "                self.url_queue.task_done()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Worker error: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "    async def process_page(self, page, response, content, url):\n",
        "        \"\"\"Process page content based on content type\"\"\"\n",
        "        try:\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "\n",
        "            if 'pdf' in content_type:\n",
        "                return await self.process_pdf_content(response, url)\n",
        "            elif any(img_type in content_type for img_type in ['image/jpeg', 'image/png', 'image/gif']):\n",
        "                return await self.process_image_content(response, url)\n",
        "            else:\n",
        "                return await self.process_html_content(page, content, url)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing page {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def should_crawl(self, url):\n",
        "      \"\"\"Determine if a URL should be crawled\"\"\"\n",
        "      try:\n",
        "        parsed = urlparse(url)\n",
        "\n",
        "        if not parsed.scheme in ['http', 'https']:\n",
        "          return False\n",
        "\n",
        "        domain = parsed.netloc\n",
        "        domain_count = sum(1 for visited in self.visited_urls\n",
        "                           if urlparse(visited).netloc == domain)\n",
        "        if domain_count >= self.max_pages_per_domain:\n",
        "          return False\n",
        "\n",
        "        exclude_patterns = [\n",
        "                r'\\.(css|js|json|xml)$',\n",
        "                r'(login|signup|logout)',\n",
        "                r'(facebook|twitter|instagram)',\n",
        "                r'\\.(jpg|jpeg|png|gif)$',  # Skip direct image links\n",
        "                r'\\/api\\/',\n",
        "                r'\\/rss\\/',\n",
        "                r'\\/feed\\/',\n",
        "                r'\\/search\\?',\n",
        "                r'\\/page\\/\\d+',\n",
        "            ]\n",
        "\n",
        "        return not any(re.search(pattern, url, re.I)\n",
        "                         for pattern in exclude_patterns)\n",
        "\n",
        "      except:\n",
        "            return False\n",
        "\n",
        "    async def process_html_content(self, page, content, url):\n",
        "        \"\"\"\n",
        "        Process HTML content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "            # Extract text content\n",
        "            text_content = ' '.join([p.get_text() for p in soup.find_all(['p', 'article', 'section'])])\n",
        "            text_content = re.sub(r'\\s+', ' ', text_content).strip()\n",
        "\n",
        "            # Get title\n",
        "            title = await page.title()\n",
        "\n",
        "            # Generate summary if content is long enough\n",
        "            summary = None\n",
        "            if len(text_content) > self.min_text_for_summary:\n",
        "                try:\n",
        "                    # Calculate dynamic max_length based on content length\n",
        "                    content_length = len(text_content.split())\n",
        "                    max_length = min(150, content_length - 50)  # At least 50 tokens shorter than content\n",
        "                    min_length = min(50, max_length - 20)  # At least 20 tokens shorter than max_length\n",
        "\n",
        "                    if max_length > min_length:\n",
        "                        summary = self.summarizer(\n",
        "                            text_content[:4096],\n",
        "                            max_length=max_length,\n",
        "                            min_length=min_length\n",
        "                        )[0]['summary_text']\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating summary: {str(e)}\")\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'type': 'html',\n",
        "                'title': title,\n",
        "                'content': text_content[:5000],  # Limit content length\n",
        "                'summary': summary,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing HTML content for {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def process_pdf_content(self, response, url):\n",
        "        \"\"\"\n",
        "        Process PDF content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            pdf_data = await response.body()\n",
        "            pdf_document = fitz.open(stream=pdf_data, filetype=\"pdf\")\n",
        "            text_content = \"\"\n",
        "\n",
        "            for page_num in range(min(pdf_document.page_count, 10)):  # Limit to first 10 pages\n",
        "                page = pdf_document[page_num]\n",
        "                text_content += page.get_text()\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'type': 'pdf',\n",
        "                'content': text_content[:5000],  # Limit content length\n",
        "                'page_count': pdf_document.page_count,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def process_image_content(self, response, url):\n",
        "        \"\"\"\n",
        "        Process image content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            image_data = await response.body()\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "            # Extract text using OCR\n",
        "            try:\n",
        "                ocr_text = pytesseract.image_to_string(image)\n",
        "            except:\n",
        "                ocr_text = \"\"\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'type': 'image',\n",
        "                'ocr_text': ocr_text,\n",
        "                'metadata': {\n",
        "                    'width': image.size[0],\n",
        "                    'height': image.size[1],\n",
        "                    'format': image.format\n",
        "                },\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def extract_urls(self, page):\n",
        "        \"\"\"\n",
        "        Extract URLs from the page\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get all links using JavaScript evaluation\n",
        "            links = await page.evaluate('''() => {\n",
        "                const links = Array.from(document.getElementsByTagName('a'));\n",
        "                return links.map(link => link.href).filter(href => href);\n",
        "            }''')\n",
        "\n",
        "            return list(set(links))  # Remove duplicates\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting URLs: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def add_result(self, result, search_query):\n",
        "        \"\"\"\n",
        "        Add processed result to the results list with relevance scoring\n",
        "        \"\"\"\n",
        "        with self.content_lock:\n",
        "            result['relevance_score'] = self.calculate_relevance(result, search_query)\n",
        "            self.results.append(result)\n",
        "            self.results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "\n",
        "    def calculate_relevance(self, result, query):\n",
        "        \"\"\"\n",
        "        Calculate relevance score for a result\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        query_terms = query.lower().split()\n",
        "\n",
        "        # Get the content to score\n",
        "        content = ''\n",
        "        if 'content' in result:\n",
        "            content = result['content'].lower()\n",
        "        elif 'ocr_text' in result:\n",
        "            content = result['ocr_text'].lower()\n",
        "\n",
        "        # Term frequency scoring\n",
        "        for term in query_terms:\n",
        "            score += content.count(term)\n",
        "\n",
        "        # Type-based boosting\n",
        "        type_boost = {\n",
        "            'pdf': 1.2,\n",
        "            'html': 1.0,\n",
        "            'image': 0.8\n",
        "        }\n",
        "        score *= type_boost.get(result['type'], 1.0)\n",
        "\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "pzUR5p4MiJAr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedSuperPoweredCrawler(SuperPoweredCrawler):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.robots_cache = {}\n",
        "        self.domain_scores = {}\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        self.ddgs = DDGS()\n",
        "\n",
        "        # Enhanced settings\n",
        "        self.concurrent_requests = 10  # Increased from 3\n",
        "        self.aiohttp_session = None\n",
        "        self.thread_pool = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def initialize(self):\n",
        "        \"\"\"Initialize crawler resources\"\"\"\n",
        "        await super().initialize_browser()\n",
        "        self.aiohttp_session = aiohttp.ClientSession(headers=self.headers)\n",
        "\n",
        "    async def cleanup(self, playwright):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        await super().cleanup(playwright)\n",
        "        if self.aiohttp_session:\n",
        "            await self.aiohttp_session.close()\n",
        "        self.thread_pool.shutdown()\n",
        "\n",
        "    async def discover_seed_urls(self, search_query, num_results=20):\n",
        "        \"\"\"Discover relevant seed URLs using DuckDuckGo search\"\"\"\n",
        "        try:\n",
        "            print(\"trying to discover seed urls\")\n",
        "            search_results = list(self.ddgs.text(\n",
        "                search_query,\n",
        "                max_results=num_results\n",
        "            ))\n",
        "            # print(\"search results are \", search_results)\n",
        "\n",
        "            urls = [result['href'] for result in search_results]\n",
        "            print(\"urls are \", urls)\n",
        "\n",
        "            # Score and filter URLs\n",
        "            scored_urls = []\n",
        "            for url in urls:\n",
        "                if await self.check_robots_txt(url):\n",
        "                    domain_score = await self.calculate_domain_score(url)\n",
        "                    scored_urls.append((url, domain_score))\n",
        "\n",
        "            # Sort by domain score and return top URLs\n",
        "            scored_urls.sort(key=lambda x: x[1], reverse=True)\n",
        "            return [url for url, _ in scored_urls[:10]]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error discovering seed URLs: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    async def check_robots_txt(self, url):\n",
        "        \"\"\"Check if URL is allowed by robots.txt\"\"\"\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "\n",
        "            if domain not in self.robots_cache:\n",
        "                robots_url = f\"{domain}/robots.txt\"\n",
        "                async with self.aiohttp_session.get(robots_url) as response:\n",
        "                    if response.status == 200:\n",
        "                        robots_content = await response.text()\n",
        "                        rp = RobotFileParser()\n",
        "                        rp.parse(robots_content.splitlines())\n",
        "                        self.robots_cache[domain] = rp\n",
        "                    else:\n",
        "                        self.robots_cache[domain] = None\n",
        "\n",
        "            rp = self.robots_cache[domain]\n",
        "            return rp is None or rp.can_fetch(self.headers['User-Agent'], url)\n",
        "\n",
        "        except Exception:\n",
        "            return True  # Allow by default if robots.txt check fails\n",
        "\n",
        "    async def calculate_domain_score(self, url):\n",
        "        \"\"\"Calculate domain authority score\"\"\"\n",
        "        if url in self.domain_scores:\n",
        "            return self.domain_scores[url]\n",
        "\n",
        "        try:\n",
        "            async with self.aiohttp_session.get(url) as response:\n",
        "                score = 1.0\n",
        "\n",
        "                # Factor 1: Response time\n",
        "                response_time = response.elapsed.total_seconds()\n",
        "                score *= max(0.5, 1 - (response_time / 5))\n",
        "\n",
        "                # Factor 2: Content quality indicators\n",
        "                if response.status == 200:\n",
        "                    content = await response.text()\n",
        "                    soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "                    # Check for structured data\n",
        "                    if soup.find(type=\"application/ld+json\"):\n",
        "                        score *= 1.2\n",
        "\n",
        "                    # Check for proper HTML structure\n",
        "                    if soup.find('article') or soup.find('main'):\n",
        "                        score *= 1.1\n",
        "\n",
        "                    # Check content length\n",
        "                    text_content = soup.get_text()\n",
        "                    if len(text_content) > 2000:\n",
        "                        score *= 1.2\n",
        "\n",
        "                self.domain_scores[url] = score\n",
        "                return score\n",
        "\n",
        "        except Exception:\n",
        "            self.domain_scores[url] = 0.5\n",
        "            return 0.5\n",
        "\n",
        "    async def start_crawl(self, seed_urls=None, search_query=None):\n",
        "        \"\"\"Enhanced crawl with automatic seed URL discovery\"\"\"\n",
        "        if not seed_urls and search_query:\n",
        "            seed_urls = await self.discover_seed_urls(search_query)\n",
        "\n",
        "        if not seed_urls:\n",
        "            raise ValueError(\"No seed URLs available for crawling\")\n",
        "\n",
        "        # await self.initialize()\n",
        "        try:\n",
        "            # Initialize URL queue with scored seed URLs\n",
        "            for url in seed_urls:\n",
        "                await self.url_queue.put((url, 0))\n",
        "\n",
        "            # Start crawler workers\n",
        "            playwright = await self.initialize()\n",
        "            workers = [self.crawler_worker(search_query) for _ in range(self.concurrent_requests)]\n",
        "            await asyncio.gather(*workers)\n",
        "\n",
        "        finally:\n",
        "            await self.cleanup(playwright)\n",
        "\n",
        "        # Sort results by relevance and return\n",
        "        self.results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        return self.results\n",
        "\n",
        "    def calculate_relevance(self, result, query):\n",
        "        \"\"\"Enhanced relevance scoring using TF-IDF\"\"\"\n",
        "        try:\n",
        "            # Get content based on result type\n",
        "            content = result.get('content', '') or result.get('ocr_text', '')\n",
        "            if not content:\n",
        "                return 0\n",
        "\n",
        "            # Create document corpus\n",
        "            corpus = [content, query]\n",
        "\n",
        "            # Calculate TF-IDF scores\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)\n",
        "            similarity = (tfidf_matrix * tfidf_matrix.T).A[0][1]\n",
        "\n",
        "            # Apply type and quality boosting\n",
        "            score = similarity * self.get_type_boost(result)\n",
        "            score *= self.get_quality_boost(result)\n",
        "\n",
        "            return score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating relevance: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def get_type_boost(self, result):\n",
        "        \"\"\"Get content type-based relevance boost\"\"\"\n",
        "        type_boost = {\n",
        "            'pdf': 1.3,  # Increased PDF boost\n",
        "            'html': 1.0,\n",
        "            'image': 0.7\n",
        "        }\n",
        "        return type_boost.get(result['type'], 1.0)\n",
        "\n",
        "    def get_quality_boost(self, result):\n",
        "        \"\"\"Calculate quality-based boost factor\"\"\"\n",
        "        boost = 1.0\n",
        "\n",
        "        # Boost based on content length\n",
        "        content = result.get('content', '') or result.get('ocr_text', '')\n",
        "        if len(content) > 5000:\n",
        "            boost *= 1.2\n",
        "\n",
        "        # Boost based on URL authority\n",
        "        url = result['url']\n",
        "        domain_score = self.domain_scores.get(urlparse(url).netloc, 0.5)\n",
        "        boost *= (1 + domain_score)\n",
        "\n",
        "        return boost"
      ],
      "metadata": {
        "id": "t_jZ7gA9CkBq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crawler = SuperPoweredCrawler()\n",
        "\n",
        "# seed_urls = [\n",
        "#     \"https://research.ibm.com/quantum-computing\",\n",
        "#     \"https://www.nature.com/subjects/quantum-physics\",\n",
        "#     \"https://www.scientificamerican.com/computing/\",\n",
        "#     \"https://www.quantum-computing.news/\",\n",
        "#     \"https://quantumcomputing.stackexchange.com/\"\n",
        "# ]\n",
        "\n",
        "# results = await crawler.start_crawl(seed_urls, \"quantum computing latest developments\")\n",
        "\n",
        "crawler = EnhancedSuperPoweredCrawler()\n",
        "results = await crawler.start_crawl(search_query=\"explain attention all you need paper\")\n",
        "print(\"Cell is completed\")"
      ],
      "metadata": {
        "id": "UD5FXfHeCyTb",
        "outputId": "501798db-6a96-448a-ef36-542e3b5b3da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to discover seed urls\n",
            "urls are  ['https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767', 'https://arxiv.org/abs/1706.03762', 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in', 'https://neuron-ai.at/attention-is-all-you-need/', 'https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937', 'https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1', 'http://research.google/pubs/attention-is-all-you-need/', 'https://medium.com/@thedatabeast/attention-is-all-you-need-summary-important-points-40769b99d6f8', 'https://www.reddit.com/r/learnmachinelearning/comments/mtegr9/attention_is_all_you_need_annotated_paper_paper/', 'https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need', 'https://storrs.io/code-walkthrough-attention-is-all-you-need/', 'https://arxiv.org/pdf/1706.03762v6', 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf', 'https://medium.com/analytics-vidhya/attention-is-all-you-need-1-3b960b7b6500', 'https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/', 'https://jalammar.github.io/illustrated-transformer/', 'https://papercopilot.com/paper/attention-is-all-you-need/', 'https://medium.com/@hyponymous/paper-summary-attention-is-all-you-need-22c2c7a5e06', 'https://arxiv.org/abs/2411.03033']\n",
            "Crawling: https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767\n",
            "Crawling: https://arxiv.org/abs/1706.03762\n",
            "Crawling: https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\n",
            "Crawling: https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in\n",
            "Crawling: https://neuron-ai.at/attention-is-all-you-need/\n",
            "Crawling: https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937\n",
            "Crawling: https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1\n",
            "Crawling: http://research.google/pubs/attention-is-all-you-need/\n",
            "Crawling: https://medium.com/@thedatabeast/attention-is-all-you-need-summary-important-points-40769b99d6f8\n",
            "Crawling: https://www.reddit.com/r/learnmachinelearning/comments/mtegr9/attention_is_all_you_need_annotated_paper_paper/\n",
            "Error response from https://www.reddit.com/r/learnmachinelearning/comments/mtegr9/attention_is_all_you_need_annotated_paper_paper/: 403\n",
            "Error processing https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Error processing https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Error processing https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937\", waiting until \"domcontentloaded\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSearch Results:\")\n",
        "for i, result in enumerate(results[:10], 1):\n",
        "    # print(result)\n",
        "    print(f\"\\n{i}. {result['url']}\")\n",
        "    print(f\"Type: {result['type']}\")\n",
        "    print(f\"Score: {result['relevance_score']:.2f}\")\n",
        "    if 'summary' in result and result['summary']:\n",
        "        print(f\"Summary: {result['summary']}\")"
      ],
      "metadata": {
        "id": "KGbbIG2zKJCL",
        "outputId": "fb451a08-111c-4f65-b805-45b5d3558eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search Results:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-df48898b6b96>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSearch Results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# print(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{i}. {result['url']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Type: {result['type']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxU0HJf75AoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# async def run_crawler():\n",
        "#     \"\"\"Main function to run the crawler\"\"\"\n",
        "#     crawler = SuperPoweredCrawler()\n",
        "\n",
        "#     seed_urls = [\n",
        "#         \"https://research.ibm.com/quantum-computing\",\n",
        "#         \"https://www.nature.com/subjects/quantum-physics\",\n",
        "#         \"https://www.scientificamerican.com/computing/\",\n",
        "#         \"https://www.quantum-computing.news/\",\n",
        "#         \"https://quantumcomputing.stackexchange.com/\"\n",
        "#     ]\n",
        "\n",
        "#     results = await crawler.start_crawl(seed_urls, \"quantum computing latest developments\")\n",
        "\n",
        "#     print(\"\\nSearch Results:\")\n",
        "#     for i, result in enumerate(results[:10], 1):\n",
        "#         print(f\"\\n{i}. {result['url']}\")\n",
        "#         print(f\"Type: {result['type']}\")\n",
        "#         print(f\"Score: {result['relevance_score']:.2f}\")\n",
        "#         if 'summary' in result and result['summary']:\n",
        "#             print(f\"Summary: {result['summary']}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     asyncio.run(run_crawler())"
      ],
      "metadata": {
        "id": "okTSfeC9FU9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Wo0eG7BIdpi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CY1WdEmwIyQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}