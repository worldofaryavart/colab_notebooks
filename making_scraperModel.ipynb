{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worldofaryavart/colab_notebooks/blob/colabnotebook/making_scraperModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright beautifulsoup4 pytesseract pillow PyMuPDF youtube_dl transformers\n",
        "!playwright install chromium\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install nest_asyncio\n",
        "!pip install duckduckgo_search\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y98lugkCaKkx",
        "outputId": "72494ad8-b872-41f3-b75e-f7835168a7cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.48.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting youtube_dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.10/dist-packages (from playwright) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading playwright-1.48.0-py3-none-manylinux1_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_dl, pytesseract, PyMuPDF, pyee, playwright\n",
            "Successfully installed PyMuPDF-1.24.13 playwright-1.48.0 pyee-12.0.0 pytesseract-0.3.13 youtube_dl-2021.12.17\n",
            "Downloading Chromium 130.0.6723.31 (playwright build v1140)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1140/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 0% 28.2s\u001b[0K\u001b[1G164.5 MiB [] 0% 19.4s\u001b[0K\u001b[1G164.5 MiB [] 0% 11.1s\u001b[0K\u001b[1G164.5 MiB [] 1% 6.5s\u001b[0K\u001b[1G164.5 MiB [] 1% 5.2s\u001b[0K\u001b[1G164.5 MiB [] 2% 4.5s\u001b[0K\u001b[1G164.5 MiB [] 2% 4.2s\u001b[0K\u001b[1G164.5 MiB [] 3% 4.1s\u001b[0K\u001b[1G164.5 MiB [] 3% 4.3s\u001b[0K\u001b[1G164.5 MiB [] 4% 4.1s\u001b[0K\u001b[1G164.5 MiB [] 4% 4.3s\u001b[0K\u001b[1G164.5 MiB [] 4% 4.4s\u001b[0K\u001b[1G164.5 MiB [] 5% 4.2s\u001b[0K\u001b[1G164.5 MiB [] 6% 4.0s\u001b[0K\u001b[1G164.5 MiB [] 6% 3.9s\u001b[0K\u001b[1G164.5 MiB [] 7% 3.8s\u001b[0K\u001b[1G164.5 MiB [] 7% 3.7s\u001b[0K\u001b[1G164.5 MiB [] 8% 3.5s\u001b[0K\u001b[1G164.5 MiB [] 9% 3.4s\u001b[0K\u001b[1G164.5 MiB [] 9% 3.3s\u001b[0K\u001b[1G164.5 MiB [] 10% 3.2s\u001b[0K\u001b[1G164.5 MiB [] 11% 3.1s\u001b[0K\u001b[1G164.5 MiB [] 11% 3.0s\u001b[0K\u001b[1G164.5 MiB [] 12% 3.0s\u001b[0K\u001b[1G164.5 MiB [] 13% 2.9s\u001b[0K\u001b[1G164.5 MiB [] 13% 2.8s\u001b[0K\u001b[1G164.5 MiB [] 14% 2.8s\u001b[0K\u001b[1G164.5 MiB [] 15% 2.7s\u001b[0K\u001b[1G164.5 MiB [] 16% 2.6s\u001b[0K\u001b[1G164.5 MiB [] 17% 2.6s\u001b[0K\u001b[1G164.5 MiB [] 18% 2.5s\u001b[0K\u001b[1G164.5 MiB [] 19% 2.4s\u001b[0K\u001b[1G164.5 MiB [] 20% 2.3s\u001b[0K\u001b[1G164.5 MiB [] 21% 2.2s\u001b[0K\u001b[1G164.5 MiB [] 22% 2.1s\u001b[0K\u001b[1G164.5 MiB [] 23% 2.1s\u001b[0K\u001b[1G164.5 MiB [] 24% 2.0s\u001b[0K\u001b[1G164.5 MiB [] 25% 2.0s\u001b[0K\u001b[1G164.5 MiB [] 26% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 27% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 28% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 29% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 30% 1.7s\u001b[0K\u001b[1G164.5 MiB [] 31% 1.7s\u001b[0K\u001b[1G164.5 MiB [] 32% 1.6s\u001b[0K\u001b[1G164.5 MiB [] 34% 1.6s\u001b[0K\u001b[1G164.5 MiB [] 35% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 36% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 38% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 39% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 40% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 41% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 42% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 43% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 45% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 46% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 47% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 48% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 49% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 51% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 52% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 53% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 54% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 55% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 57% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 58% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 59% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 60% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 61% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 62% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 63% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 64% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 65% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 66% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 67% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 68% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 69% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 70% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 71% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 72% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 73% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 74% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 75% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 76% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 77% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 78% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 79% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 80% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 81% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 82% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 83% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 84% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 86% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 88% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 91% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 92% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 94% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 97% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 130.0.6723.31 (playwright build v1140) downloaded to /root/.cache/ms-playwright/chromium-1140\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 11% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 25% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 46% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 90% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (388 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (6,049 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123653 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting duckduckgo_search\n",
            "  Downloading duckduckgo_search-6.3.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo_search) (8.1.7)\n",
            "Collecting primp>=0.6.5 (from duckduckgo_search)\n",
            "  Downloading primp-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Downloading duckduckgo_search-6.3.3-py3-none-any.whl (27 kB)\n",
            "Downloading primp-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo_search\n",
            "Successfully installed duckduckgo_search-6.3.3 primp-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from asyncio import Queue\n",
        "from bs4 import BeautifulSoup\n",
        "from playwright.async_api import async_playwright\n",
        "from transformers import pipeline\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "import io\n",
        "import aiohttp\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import warnings\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "ybCQuWaAJthn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "import aiohttp\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from urllib.robotparser import RobotFileParser\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "L-ZgI6xp4tSB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "y6SJKrkkkpx9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SuperPoweredCrawler:\n",
        "    def __init__(self):\n",
        "        self.visited_urls = set()\n",
        "        self.url_queue = asyncio.Queue()  # Changed to asyncio.Queue\n",
        "        self.results = []\n",
        "        self.content_lock = threading.Lock()\n",
        "        self.browser = None\n",
        "        self.context = None  # Added context initialization\n",
        "\n",
        "        # Initialize content extractors\n",
        "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=\"cpu\")\n",
        "        self.min_text_for_summary = 200\n",
        "\n",
        "        # Configurable crawler settings\n",
        "        self.max_depth = 3\n",
        "        self.max_pages = 50\n",
        "        self.max_pages_per_domain = 10\n",
        "        self.concurrent_requests = 3\n",
        "\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "        }\n",
        "\n",
        "    async def initialize_browser(self):\n",
        "        \"\"\"Initialize browser and context\"\"\"\n",
        "        playwright = await async_playwright().start()\n",
        "        self.browser = await playwright.chromium.launch(headless=True)  # Fixed typo in chromium\n",
        "        self.context = await self.browser.new_context(\n",
        "            viewport={'width': 1200, 'height': 800},\n",
        "            user_agent=self.headers['User-Agent'],\n",
        "            extra_http_headers=self.headers  # Fixed parameter name\n",
        "        )\n",
        "        return playwright\n",
        "\n",
        "    async def cleanup(self, playwright):\n",
        "        \"\"\"Cleanup browser resources\"\"\"\n",
        "        if self.context:\n",
        "            await self.context.close()\n",
        "        if self.browser:\n",
        "            await self.browser.close()\n",
        "        await playwright.stop()\n",
        "\n",
        "    async def start_crawl(self, seed_urls, search_query):\n",
        "        \"\"\"Start the crawling process with multiple seed URLs\"\"\"\n",
        "        print(f\"Starting crawl for query: {search_query}\")\n",
        "\n",
        "        # Initialize the queue with seed URLs\n",
        "        for url in seed_urls:\n",
        "            await self.url_queue.put((url, 0))\n",
        "\n",
        "        try:\n",
        "            playwright = await self.initialize_browser()\n",
        "            workers = [self.crawler_worker(search_query) for _ in range(self.concurrent_requests)]\n",
        "            await asyncio.gather(*workers)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during crawling: {str(e)}\")\n",
        "\n",
        "        finally:\n",
        "            await self.cleanup(playwright)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    async def crawler_worker(self, search_query):\n",
        "        \"\"\"Worker process for crawling pages\"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                if len(self.visited_urls) >= self.max_pages:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    url, depth = await asyncio.wait_for(self.url_queue.get(), timeout=10)\n",
        "                except asyncio.TimeoutError:\n",
        "                    break\n",
        "\n",
        "                if depth > self.max_depth or url in self.visited_urls:\n",
        "                    self.url_queue.task_done()\n",
        "                    continue\n",
        "\n",
        "                print(f\"Crawling: {url}\")\n",
        "\n",
        "                try:\n",
        "                    page = await self.context.new_page()\n",
        "                    response = await page.goto(\n",
        "                        url,\n",
        "                        wait_until='domcontentloaded',  # Fixed typo\n",
        "                        timeout=30000\n",
        "                    )\n",
        "\n",
        "                    if not response:\n",
        "                        print(f\"No response from {url}\")\n",
        "                        await page.close()\n",
        "                        self.url_queue.task_done()\n",
        "                        continue\n",
        "\n",
        "                    if response.status >= 400:\n",
        "                        print(f\"Error response from {url}: {response.status}\")\n",
        "                        await page.close()\n",
        "                        self.url_queue.task_done()\n",
        "                        continue\n",
        "\n",
        "                    content = await page.content()\n",
        "                    result = await self.process_page(page, response, content, url)  # Fixed parameter order\n",
        "                    if result:\n",
        "                        self.add_result(result, search_query)\n",
        "\n",
        "                    if len(self.visited_urls) < self.max_pages:\n",
        "                        new_urls = await self.extract_urls(page)\n",
        "                        for new_url in new_urls:\n",
        "                            if self.should_crawl(new_url):\n",
        "                                await self.url_queue.put((new_url, depth + 1))\n",
        "\n",
        "                    self.visited_urls.add(url)\n",
        "                    await page.close()\n",
        "                    await asyncio.sleep(2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {url}: {str(e)}\")\n",
        "                    if 'page' in locals():\n",
        "                        await page.close()\n",
        "\n",
        "                self.url_queue.task_done()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Worker error: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "    async def process_page(self, page, response, content, url):\n",
        "        \"\"\"Process page content based on content type\"\"\"\n",
        "        try:\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "\n",
        "            if 'pdf' in content_type:\n",
        "                return await self.process_pdf_content(response, url)\n",
        "            elif any(img_type in content_type for img_type in ['image/jpeg', 'image/png', 'image/gif']):\n",
        "                return await self.process_image_content(response, url)\n",
        "            else:\n",
        "                return await self.process_html_content(page, content, url)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing page {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def should_crawl(self, url):\n",
        "      \"\"\"Determine if a URL should be crawled\"\"\"\n",
        "      try:\n",
        "        parsed = urlparse(url)\n",
        "\n",
        "        if not parsed.scheme in ['http', 'https']:\n",
        "          return False\n",
        "\n",
        "        domain = parsed.netloc\n",
        "        domain_count = sum(1 for visited in self.visited_urls\n",
        "                           if urlparse(visited).netloc == domain)\n",
        "        if domain_count >= self.max_pages_per_domain:\n",
        "          return False\n",
        "\n",
        "        exclude_patterns = [\n",
        "                r'\\.(css|js|json|xml)$',\n",
        "                r'(login|signup|logout)',\n",
        "                r'(facebook|twitter|instagram)',\n",
        "                r'\\.(jpg|jpeg|png|gif)$',  # Skip direct image links\n",
        "                r'\\/api\\/',\n",
        "                r'\\/rss\\/',\n",
        "                r'\\/feed\\/',\n",
        "                r'\\/search\\?',\n",
        "                r'\\/page\\/\\d+',\n",
        "            ]\n",
        "\n",
        "        return not any(re.search(pattern, url, re.I)\n",
        "                         for pattern in exclude_patterns)\n",
        "\n",
        "      except:\n",
        "            return False\n",
        "\n",
        "    async def process_html_content(self, page, content, url):\n",
        "        \"\"\"\n",
        "        Process HTML content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "            # Extract text content\n",
        "            text_content = ' '.join([p.get_text() for p in soup.find_all(['p', 'article', 'section'])])\n",
        "            text_content = re.sub(r'\\s+', ' ', text_content).strip()\n",
        "\n",
        "            # Get title\n",
        "            title = await page.title()\n",
        "\n",
        "            # Generate summary if content is long enough\n",
        "            summary = None\n",
        "            if len(text_content) > self.min_text_for_summary:\n",
        "                try:\n",
        "                    # Calculate dynamic max_length based on content length\n",
        "                    content_length = len(text_content.split())\n",
        "                    max_length = min(150, content_length - 50)  # At least 50 tokens shorter than content\n",
        "                    min_length = min(50, max_length - 20)  # At least 20 tokens shorter than max_length\n",
        "\n",
        "                    if max_length > min_length:\n",
        "                        summary = self.summarizer(\n",
        "                            text_content[:4096],\n",
        "                            max_length=max_length,\n",
        "                            min_length=min_length\n",
        "                        )[0]['summary_text']\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating summary: {str(e)}\")\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'type': 'html',\n",
        "                'title': title,\n",
        "                'content': text_content[:5000],  # Limit content length\n",
        "                'summary': summary,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing HTML content for {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def process_pdf_content(self, response, url):\n",
        "        \"\"\"\n",
        "        Process PDF content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            pdf_data = await response.body()\n",
        "            pdf_document = fitz.open(stream=pdf_data, filetype=\"pdf\")\n",
        "            text_content = \"\"\n",
        "\n",
        "            for page_num in range(min(pdf_document.page_count, 10)):  # Limit to first 10 pages\n",
        "                page = pdf_document[page_num]\n",
        "                text_content += page.get_text()\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'type': 'pdf',\n",
        "                'content': text_content[:5000],  # Limit content length\n",
        "                'page_count': pdf_document.page_count,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def process_image_content(self, response, url):\n",
        "        \"\"\"\n",
        "        Process image content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            image_data = await response.body()\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "            # Extract text using OCR\n",
        "            try:\n",
        "                ocr_text = pytesseract.image_to_string(image)\n",
        "            except:\n",
        "                ocr_text = \"\"\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'type': 'image',\n",
        "                'ocr_text': ocr_text,\n",
        "                'metadata': {\n",
        "                    'width': image.size[0],\n",
        "                    'height': image.size[1],\n",
        "                    'format': image.format\n",
        "                },\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def extract_urls(self, page):\n",
        "        \"\"\"\n",
        "        Extract URLs from the page\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get all links using JavaScript evaluation\n",
        "            links = await page.evaluate('''() => {\n",
        "                const links = Array.from(document.getElementsByTagName('a'));\n",
        "                return links.map(link => link.href).filter(href => href);\n",
        "            }''')\n",
        "\n",
        "            return list(set(links))  # Remove duplicates\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting URLs: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def add_result(self, result, search_query):\n",
        "        \"\"\"\n",
        "        Add processed result to the results list with relevance scoring\n",
        "        \"\"\"\n",
        "        with self.content_lock:\n",
        "            result['relevance_score'] = self.calculate_relevance(result, search_query)\n",
        "            self.results.append(result)\n",
        "            self.results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "\n",
        "    def calculate_relevance(self, result, query):\n",
        "        \"\"\"\n",
        "        Calculate relevance score for a result\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        query_terms = query.lower().split()\n",
        "\n",
        "        # Get the content to score\n",
        "        content = ''\n",
        "        if 'content' in result:\n",
        "            content = result['content'].lower()\n",
        "        elif 'ocr_text' in result:\n",
        "            content = result['ocr_text'].lower()\n",
        "\n",
        "        # Term frequency scoring\n",
        "        for term in query_terms:\n",
        "            score += content.count(term)\n",
        "\n",
        "        # Type-based boosting\n",
        "        type_boost = {\n",
        "            'pdf': 1.2,\n",
        "            'html': 1.0,\n",
        "            'image': 0.8\n",
        "        }\n",
        "        score *= type_boost.get(result['type'], 1.0)\n",
        "\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "pzUR5p4MiJAr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedSuperPoweredCrawler(SuperPoweredCrawler):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.robots_cache = {}\n",
        "        self.domain_scores = {}\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        self.ddgs = DDGS()\n",
        "\n",
        "        # Enhanced settings\n",
        "        self.concurrent_requests = 10  # Increased from 3\n",
        "        self.aiohttp_session = None\n",
        "        self.thread_pool = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def initialize(self):\n",
        "        \"\"\"Initialize crawler resources\"\"\"\n",
        "        await super().initialize_browser()\n",
        "        self.aiohttp_session = aiohttp.ClientSession(headers=self.headers)\n",
        "\n",
        "    async def cleanup(self, playwright):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        await super().cleanup(playwright)\n",
        "        if self.aiohttp_session:\n",
        "            await self.aiohttp_session.close()\n",
        "        self.thread_pool.shutdown()\n",
        "\n",
        "    async def discover_seed_urls(self, search_query, num_results=20):\n",
        "        \"\"\"Discover relevant seed URLs using DuckDuckGo search\"\"\"\n",
        "        try:\n",
        "            print(\"trying to discover seed urls\")\n",
        "            search_results = list(self.ddgs.text(\n",
        "                search_query,\n",
        "                max_results=num_results\n",
        "            ))\n",
        "            # print(\"search results are \", search_results)\n",
        "\n",
        "            urls = [result['href'] for result in search_results]\n",
        "            print(\"urls are \", urls)\n",
        "\n",
        "            # Score and filter URLs\n",
        "            scored_urls = []\n",
        "            for url in urls:\n",
        "                if await self.check_robots_txt(url):\n",
        "                    domain_score = await self.calculate_domain_score(url)\n",
        "                    scored_urls.append((url, domain_score))\n",
        "\n",
        "            # Sort by domain score and return top URLs\n",
        "            scored_urls.sort(key=lambda x: x[1], reverse=True)\n",
        "            return [url for url, _ in scored_urls[:10]]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error discovering seed URLs: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    async def check_robots_txt(self, url):\n",
        "        \"\"\"Check if URL is allowed by robots.txt\"\"\"\n",
        "        try:\n",
        "            parsed_url = urlparse(url)\n",
        "            domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "\n",
        "            if domain not in self.robots_cache:\n",
        "                robots_url = f\"{domain}/robots.txt\"\n",
        "                async with self.aiohttp_session.get(robots_url) as response:\n",
        "                    if response.status == 200:\n",
        "                        robots_content = await response.text()\n",
        "                        rp = RobotFileParser()\n",
        "                        rp.parse(robots_content.splitlines())\n",
        "                        self.robots_cache[domain] = rp\n",
        "                    else:\n",
        "                        self.robots_cache[domain] = None\n",
        "\n",
        "            rp = self.robots_cache[domain]\n",
        "            return rp is None or rp.can_fetch(self.headers['User-Agent'], url)\n",
        "\n",
        "        except Exception:\n",
        "            return True  # Allow by default if robots.txt check fails\n",
        "\n",
        "    async def calculate_domain_score(self, url):\n",
        "        \"\"\"Calculate domain authority score\"\"\"\n",
        "        if url in self.domain_scores:\n",
        "            return self.domain_scores[url]\n",
        "\n",
        "        try:\n",
        "            async with self.aiohttp_session.get(url) as response:\n",
        "                score = 1.0\n",
        "\n",
        "                # Factor 1: Response time\n",
        "                response_time = response.elapsed.total_seconds()\n",
        "                score *= max(0.5, 1 - (response_time / 5))\n",
        "\n",
        "                # Factor 2: Content quality indicators\n",
        "                if response.status == 200:\n",
        "                    content = await response.text()\n",
        "                    soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "                    # Check for structured data\n",
        "                    if soup.find(type=\"application/ld+json\"):\n",
        "                        score *= 1.2\n",
        "\n",
        "                    # Check for proper HTML structure\n",
        "                    if soup.find('article') or soup.find('main'):\n",
        "                        score *= 1.1\n",
        "\n",
        "                    # Check content length\n",
        "                    text_content = soup.get_text()\n",
        "                    if len(text_content) > 2000:\n",
        "                        score *= 1.2\n",
        "\n",
        "                self.domain_scores[url] = score\n",
        "                return score\n",
        "\n",
        "        except Exception:\n",
        "            self.domain_scores[url] = 0.5\n",
        "            return 0.5\n",
        "\n",
        "    async def start_crawl(self, seed_urls=None, search_query=None):\n",
        "        \"\"\"Enhanced crawl with automatic seed URL discovery\"\"\"\n",
        "        if not seed_urls and search_query:\n",
        "            seed_urls = await self.discover_seed_urls(search_query)\n",
        "\n",
        "        if not seed_urls:\n",
        "            raise ValueError(\"No seed URLs available for crawling\")\n",
        "\n",
        "        await self.initialize()\n",
        "        try:\n",
        "            # Initialize URL queue with scored seed URLs\n",
        "            for url in seed_urls:\n",
        "                await self.url_queue.put((url, 0))\n",
        "\n",
        "            # Start crawler workers\n",
        "            playwright = await super().initialize_browser()\n",
        "            workers = [self.crawler_worker(search_query) for _ in range(self.concurrent_requests)]\n",
        "            await asyncio.gather(*workers)\n",
        "\n",
        "        finally:\n",
        "            await self.cleanup(playwright)\n",
        "\n",
        "        # Sort results by relevance and return\n",
        "        self.results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        return self.results\n",
        "\n",
        "    def calculate_relevance(self, result, query):\n",
        "        \"\"\"Enhanced relevance scoring using TF-IDF\"\"\"\n",
        "        try:\n",
        "            # Get content based on result type\n",
        "            content = result.get('content', '') or result.get('ocr_text', '')\n",
        "            if not content:\n",
        "                return 0\n",
        "\n",
        "            # Create document corpus\n",
        "            corpus = [content, query]\n",
        "\n",
        "            # Calculate TF-IDF scores\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)\n",
        "            similarity = (tfidf_matrix * tfidf_matrix.T).A[0][1]\n",
        "\n",
        "            # Apply type and quality boosting\n",
        "            score = similarity * self.get_type_boost(result)\n",
        "            score *= self.get_quality_boost(result)\n",
        "\n",
        "            return score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating relevance: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def get_type_boost(self, result):\n",
        "        \"\"\"Get content type-based relevance boost\"\"\"\n",
        "        type_boost = {\n",
        "            'pdf': 1.3,  # Increased PDF boost\n",
        "            'html': 1.0,\n",
        "            'image': 0.7\n",
        "        }\n",
        "        return type_boost.get(result['type'], 1.0)\n",
        "\n",
        "    def get_quality_boost(self, result):\n",
        "        \"\"\"Calculate quality-based boost factor\"\"\"\n",
        "        boost = 1.0\n",
        "\n",
        "        # Boost based on content length\n",
        "        content = result.get('content', '') or result.get('ocr_text', '')\n",
        "        if len(content) > 5000:\n",
        "            boost *= 1.2\n",
        "\n",
        "        # Boost based on URL authority\n",
        "        url = result['url']\n",
        "        domain_score = self.domain_scores.get(urlparse(url).netloc, 0.5)\n",
        "        boost *= (1 + domain_score)\n",
        "\n",
        "        return boost"
      ],
      "metadata": {
        "id": "t_jZ7gA9CkBq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crawler = SuperPoweredCrawler()\n",
        "\n",
        "# seed_urls = [\n",
        "#     \"https://research.ibm.com/quantum-computing\",\n",
        "#     \"https://www.nature.com/subjects/quantum-physics\",\n",
        "#     \"https://www.scientificamerican.com/computing/\",\n",
        "#     \"https://www.quantum-computing.news/\",\n",
        "#     \"https://quantumcomputing.stackexchange.com/\"\n",
        "# ]\n",
        "\n",
        "# results = await crawler.start_crawl(seed_urls, \"quantum computing latest developments\")\n",
        "\n",
        "crawler = EnhancedSuperPoweredCrawler()\n",
        "results = await crawler.start_crawl(search_query=\"explain attention all you need paper\")\n",
        "print(\"Cell is completed\")"
      ],
      "metadata": {
        "id": "UD5FXfHeCyTb",
        "outputId": "eaacbcae-15f4-4917-9bff-37306badcd23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to discover seed urls\n",
            "urls are  ['https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767', 'https://arxiv.org/abs/1706.03762', 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in', 'https://neuron-ai.at/attention-is-all-you-need/', 'https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1', 'https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937', 'https://storrs.io/code-walkthrough-attention-is-all-you-need/', 'https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need', 'http://research.google/pubs/attention-is-all-you-need/', 'https://notiona.github.io/posts/transformer-faq-eng/', 'https://medium.com/@thedatabeast/attention-is-all-you-need-summary-important-points-40769b99d6f8', 'https://www.reddit.com/r/learnmachinelearning/comments/mtegr9/attention_is_all_you_need_annotated_paper_paper/', 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf', 'https://towardsdatascience.com/the-transformer-attention-is-all-you-need-322199721720', 'https://medium.com/@malachy.moran/llm-reading-list-understanding-attention-is-all-you-need-part-ii-5ca2cd615f6b', 'https://iq.opengenus.org/attention-is-all-you-need-summary/', 'https://jalammar.github.io/illustrated-transformer/', 'https://deepai.tn/glossary/attention-is-all-you-need-explained/', 'https://medium.com/@hyponymous/paper-summary-attention-is-all-you-need-22c2c7a5e06']\n",
            "Crawling: https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767\n",
            "Crawling: https://arxiv.org/abs/1706.03762\n",
            "Crawling: https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\n",
            "Crawling: https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in\n",
            "Crawling: https://neuron-ai.at/attention-is-all-you-need/\n",
            "Crawling: https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1\n",
            "Crawling: https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937\n",
            "Crawling: https://storrs.io/code-walkthrough-attention-is-all-you-need/\n",
            "Crawling: https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need\n",
            "Crawling: http://research.google/pubs/attention-is-all-you-need/\n",
            "Error processing https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Error processing https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://btcompneuro.substack.com/p/draft-attention-is-all-you-need-in\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Error processing https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Error processing https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://medium.com/@zaiinn440/attention-is-all-you-need-the-core-idea-of-the-transformer-bbfa9a749937\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Crawling: https://arxiv.org/abs/1706.03762v1\n",
            "Crawling: https://info.arxiv.org/help/license/index.html\n",
            "Crawling: https://arxiv.org/list/cs/recent\n",
            "Crawling: https://arxiv.org/search/cs?searchtype=author&query=Kaiser,+L\n",
            "Crawling: https://replicate.com/docs/arxiv/about\n",
            "Crawling: https://influencemap.cmlab.dev/\n",
            "Crawling: https://arxiv.org/search/cs?searchtype=author&query=Polosukhin,+I\n",
            "Crawling: https://api.semanticscholar.org/arXiv:1706.03762\n",
            "Crawling: http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1706.03762&description=Attention%20Is%20All%20You%20Need\n",
            "Crawling: https://alphaxiv.org/\n",
            "Crawling: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\n",
            "Crawling: https://arxiv.org/abs/1706.03762?context=cs\n",
            "Error generating summary: Input length of decoder_input_ids is 1, but `max_length` is set to -28. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
            "Crawling: https://subscribe.sorryapp.com/24846f03/slack/new\n",
            "Crawling: https://arxiv.org/prevnext?id=1706.03762&function=next&context=cs.CL\n",
            "Crawling: https://arxiv.org/search/advanced\n",
            "Crawling: https://arxiv.org/search/cs?searchtype=author&query=Jones,+L\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x78eb3eab5510>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating summary: Input length of decoder_input_ids is 1, but `max_length` is set to -10. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
            "Crawling: https://sciencecast.org/welcome\n",
            "Crawling: https://arxiv.org/abs/1706.03762v3\n",
            "Crawling: https://dblp.uni-trier.de/search/author?author=Niki%20Parmar\n",
            "Crawling: https://www.scite.ai/\n",
            "Crawling: https://arxiv.org/abs/1706.03762v6\n",
            "Crawling: https://arxiv.org/list/cs.CL/2017-06\n",
            "Crawling: https://reddit.com/submit?url=https://arxiv.org/abs/1706.03762&title=Attention%20Is%20All%20You%20Need\n",
            "Crawling: https://arxiv.org/prevnext?id=1706.03762&function=prev&context=cs.CL\n",
            "Crawling: https://paperswithcode.com/\n",
            "Crawling: https://arxiv.org/list/cs.CL/recent\n",
            "Crawling: https://arxiv.org/abs/1706.03762v2\n",
            "Error response from https://reddit.com/submit?url=https://arxiv.org/abs/1706.03762&title=Attention%20Is%20All%20You%20Need: 403\n",
            "Crawling: https://dblp.uni-trier.de/search/author?author=Ashish%20Vaswani\n",
            "Crawling: https://arxiv.org/tb/1706.03762\n",
            "Crawling: https://arxiv.org/src/1706.03762\n",
            "Crawling: https://subscribe.sorryapp.com/24846f03/email/new\n",
            "Crawling: https://info.arxiv.org/help/mathjax.html\n",
            "Crawling: https://info.arxiv.org/help/policies/privacy_policy.html\n",
            "Error processing https://arxiv.org/src/1706.03762: Page.goto: net::ERR_ABORTED at https://arxiv.org/src/1706.03762\n",
            "Call log:\n",
            "navigating to \"https://arxiv.org/src/1706.03762\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Crawling: https://arxiv.org/auth/show-endorsers/1706.03762\n",
            "Crawling: https://arxiv.org/pdf/1706.03762\n",
            "Crawling: https://info.arxiv.org/help/contact.html\n",
            "Error generating summary: index out of range in self\n",
            "Crawling: https://arxiv.org/search/cs?searchtype=author&query=Shazeer,+N\n",
            "Crawling: https://doi.org/10.48550/arXiv.1706.03762\n",
            "Error processing https://arxiv.org/pdf/1706.03762: Page.goto: net::ERR_ABORTED at https://arxiv.org/pdf/1706.03762\n",
            "Call log:\n",
            "navigating to \"https://arxiv.org/pdf/1706.03762\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Crawling: https://arxiv.org/search/cs?searchtype=author&query=Vaswani,+A\n",
            "Crawling: https://arxiv.org/list/cs.CL/new\n",
            "Crawling: https://huggingface.co/huggingface\n",
            "Crawling: https://arxiv.org/abs/1706.03762v4\n",
            "Crawling: https://txyz.ai/\n",
            "Crawling: https://www.catalyzex.com/\n",
            "Crawling: https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17\n",
            "Crawling: https://info.arxiv.org/about/ourmembers.html\n",
            "Crawling: https://www.cornell.edu/\n",
            "Error processing https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "navigating to \"https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17\", waiting until \"domcontentloaded\"\n",
            "\n",
            "Error generating summary: index out of range in self\n",
            "Crawling: https://arxiv.org/search/cs?searchtype=author&query=Parmar,+N\n",
            "Crawling: https://arxiv.org/html/1706.03762v7\n",
            "Crawling: https://arxiv.org/show-email/f53b7360/1706.03762\n",
            "Crawling: https://arxiv.org/\n",
            "Crawling: https://dagshub.com/\n",
            "Crawling: https://arxiv.org/abs/1706.03762?context=cs.LG\n",
            "Crawling: https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit,+J\n",
            "Crawling: https://dblp.uni-trier.de/search/author?author=Jakob%20Uszkoreit\n",
            "Crawling: https://arxiv.org/abs/1706.03762v5\n",
            "Error generating summary: Input length of decoder_input_ids is 1, but `max_length` is set to -2. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
            "Cell is completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSearch Results:\")\n",
        "for i, result in enumerate(results[:10], 1):\n",
        "    # print(result)\n",
        "    print(f\"\\n{i}. {result['url']}\")\n",
        "    print(f\"Type: {result['type']}\")\n",
        "    print(f\"Score: {result['relevance_score']:.2f}\")\n",
        "    if 'summary' in result and result['summary']:\n",
        "        print(f\"Summary: {result['summary']}\")"
      ],
      "metadata": {
        "id": "KGbbIG2zKJCL",
        "outputId": "cb9c3c84-5d4e-4086-929e-4ddb6a44f0c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search Results:\n",
            "\n",
            "1. https://storrs.io/code-walkthrough-attention-is-all-you-need/\n",
            "Type: html\n",
            "Score: 0.48\n",
            "Summary: In this post I walk through the classic paper Attention Is All You Need. I’ll go through the code from the ground up, starting with attention, and incrementally build up the Transformer architecture. I focus more on the code/matrix operations than the overall reasoning of the paper. I'll be using a pytorch implementation of the original paper repository.\n",
            "\n",
            "2. https://neuron-ai.at/attention-is-all-you-need/\n",
            "Type: html\n",
            "Score: 0.42\n",
            "Summary: In the field of machine learning, the focus between parts of the inputs is referred to as attention. In the paper “Attention Is All You Need”, Ashish Vaswani et al. of Google Brain and Google Research propose an architecture which is called the Transformer. It is the first transduction model using only the attention mechanism without using sequence-aligned RNNs or convolution.\n",
            "\n",
            "3. https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need\n",
            "Type: html\n",
            "Score: 0.22\n",
            "Summary: The paper titled \"Attention Is All You Need\" introduces a new network architecture called the Transformer. It is based solely on attention mechanisms and eliminates the need for recurrent or convolutional neural networks. The authors propose this architecture as an alternative to existing sequence transduction models.\n",
            "\n",
            "4. https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\n",
            "Type: html\n",
            "Score: 0.21\n",
            "Summary: \"Attention Is All You Need\" is a 2017 research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational[5] paper in modern artificial intelligence. The name \"Transformer\" was picked because Uszkoreit liked the sound of that word.\n",
            "\n",
            "5. https://arxiv.org/auth/show-endorsers/1706.03762\n",
            "Type: html\n",
            "Score: 0.16\n",
            "\n",
            "6. https://arxiv.org/html/1706.03762v7\n",
            "Type: html\n",
            "Score: 0.11\n",
            "Summary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable.\n",
            "\n",
            "7. https://influencemap.cmlab.dev/\n",
            "Type: html\n",
            "Score: 0.10\n",
            "Summary: Influence flower visualizes citation influences among academic entities, including papers, authors, institutions, and research topics. Each edge signifies the flow of influence to and from the center node (which could be a researcher, a publication venue, an insitution, or a research field), the strength of this relation is reflected in the thickness of the edge. Red edges denote the influence the center has towards the outer entities (which can be the same types), i.e., an outer entity citing a paper by the center.\n",
            "\n",
            "8. http://research.google/pubs/attention-is-all-you-need/\n",
            "Type: html\n",
            "Score: 0.08\n",
            "Summary: Our researchers drive advancements in computer science through both fundamental and applied research. We regularly open-source projects with the broader research community and apply our developments to Google products. We make products, tools, and datasets available to everyone with the goal of building a more collaborative ecosystem.\n",
            "\n",
            "9. https://arxiv.org/search/cs?searchtype=author&query=Vaswani,+A\n",
            "Type: html\n",
            "Score: 0.06\n",
            "Summary: All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP) Model efficiency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience. In addition to inference costs, model training also have direct financial and environmental impacts.\n",
            "\n",
            "10. https://www.scite.ai/\n",
            "Type: html\n",
            "Score: 0.05\n",
            "Summary: Ask our AI Assistant or search the literature to transform the way you discover, evaluate, and understand research. Trusted by leading Universities, Publishers, and Corporations Scite goes far beyond just open-access articles. We've built the world's largest citation statement database by continually monitoring 200 million scholarly sources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# async def run_crawler():\n",
        "#     \"\"\"Main function to run the crawler\"\"\"\n",
        "#     crawler = SuperPoweredCrawler()\n",
        "\n",
        "#     seed_urls = [\n",
        "#         \"https://research.ibm.com/quantum-computing\",\n",
        "#         \"https://www.nature.com/subjects/quantum-physics\",\n",
        "#         \"https://www.scientificamerican.com/computing/\",\n",
        "#         \"https://www.quantum-computing.news/\",\n",
        "#         \"https://quantumcomputing.stackexchange.com/\"\n",
        "#     ]\n",
        "\n",
        "#     results = await crawler.start_crawl(seed_urls, \"quantum computing latest developments\")\n",
        "\n",
        "#     print(\"\\nSearch Results:\")\n",
        "#     for i, result in enumerate(results[:10], 1):\n",
        "#         print(f\"\\n{i}. {result['url']}\")\n",
        "#         print(f\"Type: {result['type']}\")\n",
        "#         print(f\"Score: {result['relevance_score']:.2f}\")\n",
        "#         if 'summary' in result and result['summary']:\n",
        "#             print(f\"Summary: {result['summary']}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     asyncio.run(run_crawler())"
      ],
      "metadata": {
        "id": "okTSfeC9FU9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Wo0eG7BIdpi"
      }
    }
  ]
}